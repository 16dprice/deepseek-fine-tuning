{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"/home/dj/Code/fine-tuning/.env\")\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "os.environ['HF_TOKEN'] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dj/snap/code/179/.local/share/hatch/env/virtual/fine-tuning/C7igg0Iu/fine-tuning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported, FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\"\n",
    "REPO_NAME = \"vector124\" # your HF username here\n",
    "MODEL_NAME = \"deepseek-r1-distill-qwen-1.5-unsloth-sft-recipe-with-reasoning-generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce GTX 1660. Max memory: 5.788 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Load the 4bit pre quantized model of deepseek and the tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "CUSTOM_TOKENS = [\"<think>\", \"</think>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": CUSTOM_TOKENS})\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# We add the LORA adapters to the model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 100/100 [00:00<00:00, 12926.23 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8131.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_style = \"\"\"Below is a request for a recipe, paired with a response for how to make the requested recipe.\n",
    "Write a response that appropriately answer the question.\n",
    "Before answering, think carefully but concisely about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are an experienced home cook who has been cooking for many years. \n",
    "Your task is to provide recipes to the user. For ingredients, your answer should \n",
    "be a bulleted list. For the instructions, your answer should also be a bulleted list. \n",
    "For other parts of your answer, feel free to format as you see fit.\n",
    "\n",
    "### Request:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    completions = examples[\"completion\"]\n",
    "    texts = []\n",
    "    for prompt,completion in zip(prompts, completions):\n",
    "        text = prompt_style.format(prompt, completion) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"vector124/recipe-data-with-reasoning\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'status', 'inserted_at', 'updated_at', '_server_id', 'system_prompt', 'prompt', 'completion', 'rating.responses', 'rating.responses.users', 'rating.responses.status', 'text'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Below is a request for a recipe, paired with a response for how to make the requested recipe.\n",
      "Write a response that appropriately answer the question.\n",
      "Before answering, think carefully but concisely about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
      "\n",
      "### Instruction:\n",
      "You are an experienced home cook who has been cooking for many years. \n",
      "Your task is to provide recipes to the user. For ingredients, your answer should \n",
      "be a bulleted list. For the instructions, your answer should also be a bulleted list. \n",
      "For other parts of your answer, feel free to format as you see fit.\n",
      "\n",
      "### Request:\n",
      "How can I make a creamy mushroom risotto that's both rich in flavor and easy to prepare for a weeknight dinner?\n",
      "\n",
      "### Response:\n",
      "<think>\n",
      "When creating a creamy mushroom risotto, it's important to focus on the balance of flavors and the texture of the dish. Risotto is known for its creamy consistency, which is achieved by slowly adding broth to the rice and stirring frequently. The choice of mushrooms can greatly enhance the flavor profile; a mix of cremini and shiitake mushrooms can provide a rich, earthy taste. Using arborio rice is essential for the creamy texture. Incorporating ingredients like garlic, onion, and Parmesan cheese will add depth to the dish. Additionally, using a bit of white wine can enhance the flavors. For ease of preparation on a weeknight, it's important to keep the recipe straightforward and limit the number of steps and ingredients. This means preparing all ingredients before starting the cooking process and using a single pan to minimize cleanup.\n",
      "</think>\n",
      "\n",
      "Here's a simple yet flavorful creamy mushroom risotto recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 cup Arborio rice\n",
      "- 2 tablespoons olive oil\n",
      "- 1 small onion, finely chopped\n",
      "- 2 cloves garlic, minced\n",
      "- 8 oz cremini mushrooms, sliced\n",
      "- 4 oz shiitake mushrooms, sliced\n",
      "- 1/2 cup dry white wine\n",
      "- 4 cups chicken or vegetable broth, kept warm\n",
      "- 1/2 cup grated Parmesan cheese\n",
      "- 2 tablespoons unsalted butter\n",
      "- Salt and pepper to taste\n",
      "- Fresh parsley, chopped (for garnish)\n",
      "\n",
      "**Instructions:**\n",
      "- Heat the olive oil in a large pan over medium heat.\n",
      "- Add the chopped onion and garlic, sauté until the onion is translucent.\n",
      "- Add the sliced mushrooms and cook until they are browned and tender.\n",
      "- Stir in the Arborio rice, ensuring it is well-coated with the oil and juices from the mushrooms.\n",
      "- Pour in the white wine and cook, stirring constantly, until the wine is absorbed.\n",
      "- Begin adding the warm broth, one ladle at a time, stirring frequently. Allow each addition to be absorbed before adding the next.\n",
      "- Continue this process until the rice is creamy and cooked to al dente, about 18-20 minutes.\n",
      "- Stir in the Parmesan cheese and butter, mixing until the risotto is creamy and well-combined.\n",
      "- Season with salt and pepper to taste.\n",
      "- Garnish with fresh parsley before serving.\n",
      "\n",
      "This recipe is designed to be both rich in flavor and easy to prepare, making it perfect for a weeknight dinner.\n",
      "<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[9][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_220086/3546885636.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 4585.74 examples/s]\n",
      "Applying chat template to train dataset (num_proc=16): 100%|██████████| 100/100 [00:02<00:00, 39.65 examples/s]\n",
      "Tokenizing train dataset (num_proc=16): 100%|██████████| 100/100 [00:02<00:00, 39.19 examples/s]\n",
      "Tokenizing train dataset (num_proc=16): 100%|██████████| 100/100 [00:00<00:00, 443.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 60,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3407,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 18,464,768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 1:33:53, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.799100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.654200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.544500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.544500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.529100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.544600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.491800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.477600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.519300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.373600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.319900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.357500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.402600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n",
    "\n",
    "with torch.cuda.device(0):  # Assuming you're using GPU 0\n",
    "    trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Below is a request for a recipe, paired with a response for how to make the requested recipe.\n",
      "Write a response that appropriately answer the question.\n",
      "Before answering, think carefully but concisely about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
      "\n",
      "### Instruction:\n",
      "You are an experienced home cook who has been cooking for many years. \n",
      "Your task is to provide recipes to the user. For ingredients, your answer should \n",
      "be a bulleted list. For the instructions, your answer should also be a bulleted list. \n",
      "For other parts of your answer, feel free to format as you see fit.\n",
      "\n",
      "### Request:\n",
      "How do I make a homemade beef stroganoff using fresh mushrooms and a creamy sauce?\n",
      "\n",
      "### Response:\n",
      "\n",
      "To address the user's request for a homemade beef stroganoff using fresh mushrooms and a creamy sauce, I need to provide a clear and detailed recipe. The recipe should include ingredients such as fresh mushrooms, beef, onions, garlic, tomatoes, and a creamy sauce made from butter, flour, and a secret ingredient. The steps should guide the user through the preparation process, ensuring that all components are combined in the correct proportions and that the sauce is simmered properly to achieve a creamy texture. Additionally, I should mention optional additions such as herbs or garnishes to enhance the dish. The final answer should be a well-structured recipe that can be followed step-by-step without confusion.\n",
      "</think>\n",
      "\n",
      "To address the user's request for a homemade beef stroganoff using fresh mushrooms and a creamy sauce, I need to provide a clear and detailed recipe. The recipe should include ingredients such as fresh mushrooms, beef, onions, garlic, tomatoes, and a creamy sauce made from butter, flour, and a secret ingredient. The steps should guide the user through the preparation process, ensuring all components are combined in the correct proportions and the sauce is simmered properly to achieve a creamy texture. Additionally, optional additions like herbs or garnishes can enhance the dish. The final answer should be a well-structured recipe that can be followed step-by-step without confusion.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "question = \"How do I make a homemade beef stroganoff using fresh mushrooms and a creamy sauce?\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    \n",
    "    # max_new_tokens=2048,\n",
    "    # use_cache=True,\n",
    "\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 10.7 out of 31.24 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 24/28 [00:00<00:00, 36.28it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|██████████| 28/28 [00:00<00:00, 31.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Save to the local directory and push it to the Hub\n",
    "model.save_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained(MODEL_NAME)\n",
    "model.save_pretrained_merged(MODEL_NAME, tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.bin: 100%|██████████| 74.0M/74.0M [00:02<00:00, 30.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/vector124/deepseek-r1-distill-qwen-1.5-unsloth-sft-recipe-with-reasoning-generator\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = f\"{REPO_NAME}/{MODEL_NAME}\"\n",
    "model.push_to_hub(fine_tuned_model, safe_serialization=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
