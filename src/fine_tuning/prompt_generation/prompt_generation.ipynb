{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"/home/dj/Code/fine-tuning/.env\")\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "os.environ['HF_TOKEN'] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tuning.clients.openai_api_client import OpenAIApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_input():\n",
    "    openai_client = OpenAIApiClient()\n",
    "    completion = openai_client.chat_completion(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\":\n",
    "                    \"You are an experienced home cook who has been cooking for many years. \" +\n",
    "                    \"Your task is to provide an example of a question for a full recipe that a reasonably \" +\n",
    "                    \"experienced home cook would ask for. Please provide exactly one example as if you were the user.\" +\n",
    "                    \"\\n\\n\" +\n",
    "                    \"Exapmles include:\\n\" +\n",
    "                    \"- What are the steps and ingredients for making a classic homemade lasagna from scratch?\\n\" +\n",
    "                    \"- Can you recommend me a recipe that includes chicken and some kind of cheese?\\n\" +\n",
    "                    \"- Is there something I can make out of the garlic, pepperonis, and vegetables I have on hand?\\n\" +\n",
    "                    \"\\n\\n\" +\n",
    "                    \"Be varied in your answer. Try to match the style of the example questions but don't get stuck on specifically those ingredients.\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.9,\n",
    "        model=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an experienced home cook who has been cooking for many years. \\\n",
    "Your task is to provide recipes to the user. For ingredients, your answer should \\\n",
    "be a bulleted list. For the instructions, your answer should also be a bulleted list. \\\n",
    "For other parts of your answer, feel free to format as you see fit. \\\n",
    "\\n\\n\n",
    "For each of your answers, respond with a detailed chain-of-thought for how you arrived \\\n",
    "at your answer. The chain-of-thought (COT) reasoning should be contained within two \\\n",
    "HTML like `think` tags. Following those tags, you should include your actual, final answer. \\\n",
    "An example of what your output should look like is given below.\n",
    "\n",
    "==== BEGIN EXAMPLE ====\n",
    "\n",
    "<think>\n",
    "COT REASONING HERE\n",
    "</think>\n",
    "ACTUAL ANSWER HERE\n",
    "\n",
    "==== END EXAMPLE ====\n",
    "\"\"\"\n",
    "\n",
    "def generate_answer(user_input: str):\n",
    "    openai_client = OpenAIApiClient()\n",
    "    completion = openai_client.chat_completion(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT,\n",
    "                    \n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tuning.clients.simple_argilla_dataset_client import SimpleArgillaDatasetClient, create_dataset\n",
    "\n",
    "dataset = SimpleArgillaDatasetClient(name=\"recipe-data-with-reasoning\", workspace=\"vector124\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def run_in_parallel(fns: list):\n",
    "  proc = []\n",
    "  for fn in fns:\n",
    "    try:\n",
    "      p = Process(target=fn)\n",
    "      p.start()\n",
    "      proc.append(p)\n",
    "    except:\n",
    "      pass\n",
    "  for p in proc:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "def get_ex():\n",
    "    prompt = generate_user_input()\n",
    "    completion = generate_answer(prompt)\n",
    "\n",
    "    record = rg.Record(\n",
    "        fields={\n",
    "            \"system_prompt\": SYSTEM_PROMPT,\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dataset.upsert_records([record])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.62batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.83batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.67batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.86batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.75batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.87batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.91batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.84batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.55batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.79batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.76batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.45batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.31batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.90batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.91batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.83batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.73batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.76batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.79batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  5.20batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  3.89batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.06batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.85batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.61batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  4.58batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  5.16batch/s]\n",
      "Sending records...: 100%|██████████| 1/1 [00:00<00:00,  3.87batch/s]\n"
     ]
    }
   ],
   "source": [
    "run_in_parallel([get_ex for _ in range(27)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
